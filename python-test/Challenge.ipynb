{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge consists of two sets of files:\n",
    "\n",
    "* `X.CSV` for various values of `X`: contains data similar to what you were to produce for the C++ part of this challenge. Specifically, each file contains the average, min, max, and standard deviation of the total acceleration in 10 second long time windows for a single video. The `start` and `end` columns indicate the bounds of the time window in milliseconds from the beginning of the video.\n",
    "* `X.jumps.csv` for various values of `X`: these files go with the matching `X.CSV` files and indicate when a kite surfer jumped. That is, if one row of `X.jumps.csv` has `start == 53280` and `end == 55540` then a kite surfer jumped, leaving the water at 53280 milliseconds from the beginning of the video and landing back on the water at 55540 milliseconds from the beginning of the video.\n",
    "\n",
    "The goal is to be able to predict time windows which contain kite jumps. This Jupyter Notebook is a (bad) attempt at building a model to do that. Your challenge is to identify the various ways this attempt could be improved. You do not need to actual fix the code (though that is permitted). You can simply add new Markdown cells to the notebook indicating the mistakes you observe and suggesting improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os.path\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import sklearn.model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir='challenge_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the forces and jump data into dictionaries whose keys are the names of the jump files and whose values are the Pandas DataFrame instances holding the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "force_files = glob.glob(os.path.join(data_dir, '*.CSV'))\n",
    "jump_files = glob.glob(os.path.join(data_dir, '*.jumps.csv'))\n",
    "\n",
    "forces = {}\n",
    "for ff in force_files:\n",
    "    forces[ff] = pd.read_csv(ff)\n",
    "jumps = {}\n",
    "for ff, jf in zip(force_files, jump_files):\n",
    "    # Use the forces filename as the key so we can easily match the jump times with\n",
    "    # the corresponding forces\n",
    "    jumps[ff] = pd.read_csv(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['challenge_data/V3.CSV',\n",
       " 'challenge_data/V4.CSV',\n",
       " 'challenge_data/V1.CSV',\n",
       " 'challenge_data/V2.CSV']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forces.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>start</th>\n",
       "      <th>stddev</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.193093</td>\n",
       "      <td>3.077215</td>\n",
       "      <td>0.120284</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568452</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.188233</td>\n",
       "      <td>3.077215</td>\n",
       "      <td>0.120284</td>\n",
       "      <td>500</td>\n",
       "      <td>0.572365</td>\n",
       "      <td>10500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.188903</td>\n",
       "      <td>3.077215</td>\n",
       "      <td>0.120284</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.575058</td>\n",
       "      <td>11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.194125</td>\n",
       "      <td>3.077215</td>\n",
       "      <td>0.120284</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.577237</td>\n",
       "      <td>11500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.221625</td>\n",
       "      <td>3.113024</td>\n",
       "      <td>0.120284</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.595027</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        avg       max       min  start    stddev    end\n",
       "0  1.193093  3.077215  0.120284      0  0.568452  10000\n",
       "1  1.188233  3.077215  0.120284    500  0.572365  10500\n",
       "2  1.188903  3.077215  0.120284   1000  0.575058  11000\n",
       "3  1.194125  3.077215  0.120284   1500  0.577237  11500\n",
       "4  1.221625  3.113024  0.120284   2000  0.595027  12000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forces['challenge_data/V1.CSV'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['challenge_data/V3.CSV',\n",
       " 'challenge_data/V4.CSV',\n",
       " 'challenge_data/V1.CSV',\n",
       " 'challenge_data/V2.CSV']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jumps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295800</td>\n",
       "      <td>298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>379600</td>\n",
       "      <td>384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>558300</td>\n",
       "      <td>562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1056300</td>\n",
       "      <td>1060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1125400</td>\n",
       "      <td>1129500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Start      End\n",
       "0   295800   298000\n",
       "1   379600   384300\n",
       "2   558300   562800\n",
       "3  1056300  1060700\n",
       "4  1125400  1129500"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jumps['challenge_data/V1.CSV'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to join the computed data (the summary statistics for each time window) against the jumps data so we have a target for supervised learning. Specifically, if the window for a row contains a jump then our target is `True`. Otherwise it is `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in forces.keys():\n",
    "    cur_f = forces[k]\n",
    "    cur_f['Target'] = False\n",
    "    jt = jumps[k]\n",
    "    for i in range(jt.shape[0]):\n",
    "        start = jt.Start.iloc[i]\n",
    "        end = jt.End.iloc[i]\n",
    "        cur_f.loc[(cur_f.start <= start) & (cur_f.end >= end), 'Target'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295800</td>\n",
       "      <td>298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>379600</td>\n",
       "      <td>384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>558300</td>\n",
       "      <td>562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1056300</td>\n",
       "      <td>1060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1125400</td>\n",
       "      <td>1129500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Start      End\n",
       "0   295800   298000\n",
       "1   379600   384300\n",
       "2   558300   562800\n",
       "3  1056300  1060700\n",
       "4  1125400  1129500"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jumps['challenge_data/V1.CSV'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>start</th>\n",
       "      <th>stddev</th>\n",
       "      <th>end</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.380238</td>\n",
       "      <td>4.863788</td>\n",
       "      <td>0.304278</td>\n",
       "      <td>40500</td>\n",
       "      <td>0.682105</td>\n",
       "      <td>50500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.361738</td>\n",
       "      <td>4.863788</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>41000</td>\n",
       "      <td>0.679024</td>\n",
       "      <td>51000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.328504</td>\n",
       "      <td>4.863788</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>41500</td>\n",
       "      <td>0.656603</td>\n",
       "      <td>51500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.326717</td>\n",
       "      <td>4.863788</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>42000</td>\n",
       "      <td>0.656298</td>\n",
       "      <td>52000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.345357</td>\n",
       "      <td>4.863788</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>42500</td>\n",
       "      <td>0.657399</td>\n",
       "      <td>52500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.347320</td>\n",
       "      <td>4.863788</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>43000</td>\n",
       "      <td>0.675417</td>\n",
       "      <td>53000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.329462</td>\n",
       "      <td>4.440003</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>43500</td>\n",
       "      <td>0.628885</td>\n",
       "      <td>53500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.284885</td>\n",
       "      <td>3.784215</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>44000</td>\n",
       "      <td>0.575518</td>\n",
       "      <td>54000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.271766</td>\n",
       "      <td>3.784215</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>44500</td>\n",
       "      <td>0.556836</td>\n",
       "      <td>54500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.252970</td>\n",
       "      <td>3.784215</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>45000</td>\n",
       "      <td>0.533204</td>\n",
       "      <td>55000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.247687</td>\n",
       "      <td>3.784215</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>45500</td>\n",
       "      <td>0.540375</td>\n",
       "      <td>55500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.231313</td>\n",
       "      <td>3.784215</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>46000</td>\n",
       "      <td>0.530266</td>\n",
       "      <td>56000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.202928</td>\n",
       "      <td>3.255621</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>46500</td>\n",
       "      <td>0.495356</td>\n",
       "      <td>56500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.191955</td>\n",
       "      <td>3.255621</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>47000</td>\n",
       "      <td>0.504564</td>\n",
       "      <td>57000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.196527</td>\n",
       "      <td>3.255621</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>47500</td>\n",
       "      <td>0.513857</td>\n",
       "      <td>57500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.205360</td>\n",
       "      <td>3.255621</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>48000</td>\n",
       "      <td>0.529780</td>\n",
       "      <td>58000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.190495</td>\n",
       "      <td>3.194549</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>48500</td>\n",
       "      <td>0.514660</td>\n",
       "      <td>58500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.187943</td>\n",
       "      <td>3.194549</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>49000</td>\n",
       "      <td>0.512845</td>\n",
       "      <td>59000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.195151</td>\n",
       "      <td>3.194549</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>49500</td>\n",
       "      <td>0.523688</td>\n",
       "      <td>59500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.198596</td>\n",
       "      <td>3.194549</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>50000</td>\n",
       "      <td>0.525463</td>\n",
       "      <td>60000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.193465</td>\n",
       "      <td>3.194549</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>50500</td>\n",
       "      <td>0.515858</td>\n",
       "      <td>60500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.194427</td>\n",
       "      <td>3.194549</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>51000</td>\n",
       "      <td>0.525453</td>\n",
       "      <td>61000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.223584</td>\n",
       "      <td>3.249643</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>51500</td>\n",
       "      <td>0.552576</td>\n",
       "      <td>61500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.241528</td>\n",
       "      <td>3.249643</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>52000</td>\n",
       "      <td>0.570250</td>\n",
       "      <td>62000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.240090</td>\n",
       "      <td>3.249643</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>52500</td>\n",
       "      <td>0.579588</td>\n",
       "      <td>62500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.259608</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>53000</td>\n",
       "      <td>0.600303</td>\n",
       "      <td>63000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.243202</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>53500</td>\n",
       "      <td>0.598368</td>\n",
       "      <td>63500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.258249</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>54000</td>\n",
       "      <td>0.599998</td>\n",
       "      <td>64000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.258051</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>54500</td>\n",
       "      <td>0.614246</td>\n",
       "      <td>64500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.263149</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>55000</td>\n",
       "      <td>0.622155</td>\n",
       "      <td>65000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.261810</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>55500</td>\n",
       "      <td>0.620632</td>\n",
       "      <td>65500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.260835</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>56000</td>\n",
       "      <td>0.619735</td>\n",
       "      <td>66000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.249030</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.272211</td>\n",
       "      <td>56500</td>\n",
       "      <td>0.616770</td>\n",
       "      <td>66500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.268301</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.284203</td>\n",
       "      <td>57000</td>\n",
       "      <td>0.632263</td>\n",
       "      <td>67000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.268578</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.230933</td>\n",
       "      <td>57500</td>\n",
       "      <td>0.640421</td>\n",
       "      <td>67500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.260273</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.230933</td>\n",
       "      <td>58000</td>\n",
       "      <td>0.626636</td>\n",
       "      <td>68000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.250965</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.230933</td>\n",
       "      <td>58500</td>\n",
       "      <td>0.622654</td>\n",
       "      <td>68500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.254988</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.230933</td>\n",
       "      <td>59000</td>\n",
       "      <td>0.622593</td>\n",
       "      <td>69000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.238871</td>\n",
       "      <td>3.693975</td>\n",
       "      <td>0.230933</td>\n",
       "      <td>59500</td>\n",
       "      <td>0.611240</td>\n",
       "      <td>69500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          avg       max       min  start    stddev    end Target\n",
       "81   1.380238  4.863788  0.304278  40500  0.682105  50500  False\n",
       "82   1.361738  4.863788  0.274618  41000  0.679024  51000  False\n",
       "83   1.328504  4.863788  0.274618  41500  0.656603  51500  False\n",
       "84   1.326717  4.863788  0.274618  42000  0.656298  52000  False\n",
       "85   1.345357  4.863788  0.274618  42500  0.657399  52500  False\n",
       "86   1.347320  4.863788  0.274618  43000  0.675417  53000  False\n",
       "87   1.329462  4.440003  0.274618  43500  0.628885  53500  False\n",
       "88   1.284885  3.784215  0.274618  44000  0.575518  54000  False\n",
       "89   1.271766  3.784215  0.274618  44500  0.556836  54500  False\n",
       "90   1.252970  3.784215  0.274618  45000  0.533204  55000  False\n",
       "91   1.247687  3.784215  0.274618  45500  0.540375  55500  False\n",
       "92   1.231313  3.784215  0.274618  46000  0.530266  56000  False\n",
       "93   1.202928  3.255621  0.274618  46500  0.495356  56500  False\n",
       "94   1.191955  3.255621  0.272211  47000  0.504564  57000  False\n",
       "95   1.196527  3.255621  0.272211  47500  0.513857  57500  False\n",
       "96   1.205360  3.255621  0.272211  48000  0.529780  58000  False\n",
       "97   1.190495  3.194549  0.272211  48500  0.514660  58500  False\n",
       "98   1.187943  3.194549  0.272211  49000  0.512845  59000  False\n",
       "99   1.195151  3.194549  0.272211  49500  0.523688  59500  False\n",
       "100  1.198596  3.194549  0.272211  50000  0.525463  60000  False\n",
       "101  1.193465  3.194549  0.272211  50500  0.515858  60500  False\n",
       "102  1.194427  3.194549  0.272211  51000  0.525453  61000  False\n",
       "103  1.223584  3.249643  0.272211  51500  0.552576  61500  False\n",
       "104  1.241528  3.249643  0.272211  52000  0.570250  62000  False\n",
       "105  1.240090  3.249643  0.272211  52500  0.579588  62500  False\n",
       "106  1.259608  3.693975  0.272211  53000  0.600303  63000  False\n",
       "107  1.243202  3.693975  0.272211  53500  0.598368  63500  False\n",
       "108  1.258249  3.693975  0.272211  54000  0.599998  64000  False\n",
       "109  1.258051  3.693975  0.272211  54500  0.614246  64500  False\n",
       "110  1.263149  3.693975  0.272211  55000  0.622155  65000  False\n",
       "111  1.261810  3.693975  0.272211  55500  0.620632  65500  False\n",
       "112  1.260835  3.693975  0.272211  56000  0.619735  66000  False\n",
       "113  1.249030  3.693975  0.272211  56500  0.616770  66500  False\n",
       "114  1.268301  3.693975  0.284203  57000  0.632263  67000  False\n",
       "115  1.268578  3.693975  0.230933  57500  0.640421  67500  False\n",
       "116  1.260273  3.693975  0.230933  58000  0.626636  68000  False\n",
       "117  1.250965  3.693975  0.230933  58500  0.622654  68500  False\n",
       "118  1.254988  3.693975  0.230933  59000  0.622593  69000  False\n",
       "119  1.238871  3.693975  0.230933  59500  0.611240  69500  False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we set our target correctly\n",
    "f1 = forces['challenge_data/V1.CSV']\n",
    "f1[(f1.start > 40000) & (f1.start < 60000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now concatenate things into one big data set\n",
    "all_data = pd.concat(forces.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>start</th>\n",
       "      <th>stddev</th>\n",
       "      <th>end</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.979240</td>\n",
       "      <td>1.434954</td>\n",
       "      <td>0.654423</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061798</td>\n",
       "      <td>10000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978753</td>\n",
       "      <td>1.434954</td>\n",
       "      <td>0.654423</td>\n",
       "      <td>500</td>\n",
       "      <td>0.060129</td>\n",
       "      <td>10500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.976366</td>\n",
       "      <td>1.434954</td>\n",
       "      <td>0.654423</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.059742</td>\n",
       "      <td>11000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.976729</td>\n",
       "      <td>1.434954</td>\n",
       "      <td>0.654423</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.053761</td>\n",
       "      <td>11500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975602</td>\n",
       "      <td>1.434954</td>\n",
       "      <td>0.479002</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.060523</td>\n",
       "      <td>12000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        avg       max       min  start    stddev    end Target\n",
       "0  0.979240  1.434954  0.654423      0  0.061798  10000  False\n",
       "1  0.978753  1.434954  0.654423    500  0.060129  10500  False\n",
       "2  0.976366  1.434954  0.654423   1000  0.059742  11000  False\n",
       "3  0.976729  1.434954  0.654423   1500  0.053761  11500  False\n",
       "4  0.975602  1.434954  0.479002   2000  0.060523  12000  False"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8397, 7)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8397"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x.shape[0] for x in forces.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the data into training and test data sets.\n",
    "\n",
    "**SPENCER**: Calling your test set \"valid\" is somewhat unorthodox... Changing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = ms.train_test_split(all_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>start</th>\n",
       "      <th>stddev</th>\n",
       "      <th>end</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>1.443083</td>\n",
       "      <td>5.838410</td>\n",
       "      <td>0.336497</td>\n",
       "      <td>32500</td>\n",
       "      <td>0.716005</td>\n",
       "      <td>42500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>1.183809</td>\n",
       "      <td>3.241373</td>\n",
       "      <td>0.362419</td>\n",
       "      <td>121000</td>\n",
       "      <td>0.467846</td>\n",
       "      <td>131000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>1.227089</td>\n",
       "      <td>3.212756</td>\n",
       "      <td>0.572236</td>\n",
       "      <td>670500</td>\n",
       "      <td>0.393725</td>\n",
       "      <td>680500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>1.107248</td>\n",
       "      <td>2.220643</td>\n",
       "      <td>0.290170</td>\n",
       "      <td>106000</td>\n",
       "      <td>0.334737</td>\n",
       "      <td>116000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>1.693326</td>\n",
       "      <td>8.219544</td>\n",
       "      <td>0.265194</td>\n",
       "      <td>524500</td>\n",
       "      <td>1.249571</td>\n",
       "      <td>534500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           avg       max       min   start    stddev     end Target\n",
       "6644  1.443083  5.838410  0.336497   32500  0.716005   42500  False\n",
       "5310  1.183809  3.241373  0.362419  121000  0.467846  131000  False\n",
       "1341  1.227089  3.212756  0.572236  670500  0.393725  680500  False\n",
       "5280  1.107248  2.220643  0.290170  106000  0.334737  116000  False\n",
       "4127  1.693326  8.219544  0.265194  524500  1.249571  534500  False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>start</th>\n",
       "      <th>stddev</th>\n",
       "      <th>end</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>1.223360</td>\n",
       "      <td>3.633863</td>\n",
       "      <td>0.699267</td>\n",
       "      <td>422000</td>\n",
       "      <td>0.403433</td>\n",
       "      <td>432000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>1.567193</td>\n",
       "      <td>4.543819</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>213500</td>\n",
       "      <td>0.738849</td>\n",
       "      <td>223500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5412</th>\n",
       "      <td>1.170742</td>\n",
       "      <td>2.930905</td>\n",
       "      <td>0.183172</td>\n",
       "      <td>172000</td>\n",
       "      <td>0.449847</td>\n",
       "      <td>182000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8287</th>\n",
       "      <td>1.131793</td>\n",
       "      <td>4.524100</td>\n",
       "      <td>0.236222</td>\n",
       "      <td>854000</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>864000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5384</th>\n",
       "      <td>1.161403</td>\n",
       "      <td>2.603983</td>\n",
       "      <td>0.237976</td>\n",
       "      <td>158000</td>\n",
       "      <td>0.446781</td>\n",
       "      <td>168000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           avg       max       min   start    stddev     end Target\n",
       "5912  1.223360  3.633863  0.699267  422000  0.403433  432000  False\n",
       "427   1.567193  4.543819  0.227626  213500  0.738849  223500  False\n",
       "5412  1.170742  2.930905  0.183172  172000  0.449847  182000  False\n",
       "8287  1.131793  4.524100  0.236222  854000  0.421100  864000  False\n",
       "5384  1.161403  2.603983  0.237976  158000  0.446781  168000  False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train ratio:', 0.03930326038409996)\n",
      "('Test ratio:', 0.033928571428571426)\n"
     ]
    }
   ],
   "source": [
    "frac_true = np.sum(train.values[:,-1]/np.float_(train.values.shape[0]))\n",
    "print (\"Train ratio:\", frac_true)\n",
    "print(\"Test ratio:\",np.sum(test.values[:,-1])/np.float_(test.values.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6717, 7)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1680, 7)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see if we can train a neural network on this data.\n",
    "\n",
    "**SPENCER**: Sigmoids can be tempermental... ReLUs generally are better. Also your features are not in the same range of values which can lead to issues.\n",
    "\n",
    "Performing some normalization to the input features would be a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(4,))\n",
    "l1 = Dense(10, activation='sigmoid')(inputs)\n",
    "l2 = Dense(15, activation='sigmoid')(l1)\n",
    "out = Dense(1, activation='sigmoid')(l2)\n",
    "model = Model(inputs=inputs, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_data_and_target(df):\n",
    "    \"\"\"Given either our training or test data set return a pair of (data, targets) where data is just the\n",
    "    columns that should be input and targets are just the corresponding targets.\"\"\"\n",
    "    data = df[['avg', 'min', 'max', 'stddev']]\n",
    "    targets = df.Target\n",
    "    return (data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_d, train_t = to_data_and_target(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>1.443083</td>\n",
       "      <td>0.336497</td>\n",
       "      <td>5.838410</td>\n",
       "      <td>0.716005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>1.183809</td>\n",
       "      <td>0.362419</td>\n",
       "      <td>3.241373</td>\n",
       "      <td>0.467846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>1.227089</td>\n",
       "      <td>0.572236</td>\n",
       "      <td>3.212756</td>\n",
       "      <td>0.393725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>1.107248</td>\n",
       "      <td>0.290170</td>\n",
       "      <td>2.220643</td>\n",
       "      <td>0.334737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>1.693326</td>\n",
       "      <td>0.265194</td>\n",
       "      <td>8.219544</td>\n",
       "      <td>1.249571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           avg       min       max    stddev\n",
       "6644  1.443083  0.336497  5.838410  0.716005\n",
       "5310  1.183809  0.362419  3.241373  0.467846\n",
       "1341  1.227089  0.572236  3.212756  0.393725\n",
       "5280  1.107248  0.290170  2.220643  0.334737\n",
       "4127  1.693326  0.265194  8.219544  1.249571"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t.head()\n",
    "np.sum(train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6045 samples, validate on 672 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 0.2948 - acc: 0.8961 - val_loss: 0.1628 - val_acc: 0.9643\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.1693 - acc: 0.9603 - val_loss: 0.1554 - val_acc: 0.9643\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.1673 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.1672 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1542 - val_acc: 0.9643\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.1671 - acc: 0.9603 - val_loss: 0.1543 - val_acc: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0f7ec43d0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optimizers.SGD(lr=0.02, momentum=0.2, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=train_d.values, y=train_t.values, validation_split=0.1, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like things were still improving after 50 epoch so maybe we should try more.\n",
    "\n",
    "**SPENCER**: Pretty difficult to assess performance of a model if you're only looking at the change in the training set performance -- you should be able to achieve perfect performance on your training set with a sufficiently complex model.\n",
    "\n",
    "By adding a validation split we can keep an eye on overfitting and see whether we're actually getting any improvement.\n",
    "\n",
    "Also, since positives only make up 3% of the data, you're going to get at least 96+% accuracy by just predicting FALSE every time. \n",
    "\n",
    "Some class weighting or sampling scheme to account for this could help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6045 samples, validate on 672 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.1664 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.1663 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0f567e410>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optimizers.SGD(lr=0.02, momentum=0.2, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=train_d.values, y=train_t.values, epochs=100, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks like even after 100 epochs things were still improving, though slowly. So maybe we should try more and/or increase the learning rate and/or decrease the decay. Let's try something like that.\n",
    "\n",
    "**SPENCER**: Parameters should be altered one at a time to measure their effect. Particularly in the case of interrelated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6045 samples, validate on 672 samples\n",
      "Epoch 1/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1544 - val_acc: 0.9643\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1545 - val_acc: 0.9643\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.1670 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1546 - val_acc: 0.9643\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1547 - val_acc: 0.9643\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.1669 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1553 - val_acc: 0.9643\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.1668 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.1666 - acc: 0.9603 - val_loss: 0.1554 - val_acc: 0.9643\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1553 - val_acc: 0.9643\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.1666 - acc: 0.9603 - val_loss: 0.1548 - val_acc: 0.9643\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1553 - val_acc: 0.9643\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.1666 - acc: 0.9603 - val_loss: 0.1553 - val_acc: 0.9643\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1552 - val_acc: 0.9643\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.1666 - acc: 0.9603 - val_loss: 0.1549 - val_acc: 0.9643\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.1667 - acc: 0.9603 - val_loss: 0.1551 - val_acc: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0f70ee310>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optimizers.SGD(lr=0.04, momentum=0.2, decay=1e-7)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=train_d.values, y=train_t.values, epochs=200, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! Let's assess accuracy on our validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680/1680 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_d, test_t = to_data_and_target(test)\n",
    "model.evaluate(test_d.values, test_t.values)\n",
    "\n",
    "#Percentage of jumps correctly identified, assuming a 0.5 cut\n",
    "p = model.predict(test_d.values)\n",
    "np.sum([1 for it, thing in enumerate(p>0.5) if thing == test_t.values[it] and test_t.values[it]==True])/np.float_(test_d.values.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Better than 95% accuracy our validation set. This is a good model!\n",
    "\n",
    "**SPENCER**: We see that assuming a threshold for binary classification of 0.5, we actually identify 0 of the jumps correctly. Since the goal here is to learn to identify jumps, we're not doing too great. Monitoring a different metric (like recall) would help give a clearer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
